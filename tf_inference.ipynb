{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Jupyter Notebook as a guide to run your trained model in inference mode\n",
    "\n",
    "created by Anton Morgunov\n",
    "\n",
    "inspired by [tensorflow object detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first step is going to specify which unit you are going to work with for inference. Select between GPU or CPU and follow the below instructions for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # importing OS in order to make GPU visible\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # do not change anything in here\n",
    "\n",
    "# specify which device you want to work on.\n",
    "# Use \"-1\" to work on a CPU. Default value \"0\" stands for the 1st GPU that will be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # TODO: specify your computational device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # import tensorflow\n",
    "\n",
    "# checking that GPU is found\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other import\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will import import scripts that were already provided by Tensorflow API. **Make sure that Tensorflow is your current working directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # importyng sys in order to access scripts located in a different folder\n",
    "\n",
    "path2scripts = 'models/research/' # TODO: provide pass to the research folder\n",
    "sys.path.insert(0, path2scripts) # making scripts in models/research available for import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all scripts that will be needed to export your model and use it for inference\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import and build your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: your current working directory should be Tensorflow.\n",
    "\n",
    "# TODO: specify two pathes: to the pipeline.config file and to the folder with trained model.\n",
    "path2config ='Tensorflow/workspace/exported-models/my_ssd_mobnet/pipeline.config'\n",
    "path2model = 'Tensorflow/workspace/exported-models/my_ssd_mobnet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change anything in this cell\n",
    "configs = config_util.get_configs_from_pipeline_file(path2config) # importing config\n",
    "model_config = configs['model'] # recreating model config\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False) # importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e9341b07f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(path2model, 'ckpt-0')).expect_partial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, path to label map should be provided. Category index will be created based on labal map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2label_map = 'Tensorflow/workspace/annotations/label_map.pbtxt' # TODO: provide a path to the label map file\n",
    "category_index = label_map_util.create_category_index_from_labelmap(path2label_map,use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a few supporting functions will be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fn(image):\n",
    "    \"\"\"\n",
    "    Detect objects in image.\n",
    "    \n",
    "    Args:\n",
    "      image: (tf.tensor): 4D input image\n",
    "      \n",
    "    Returs:\n",
    "      detections (dict): predictions that model made\n",
    "    \"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next function is the one that you can use to run inference and plot results an an input image:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inference_with_plot(path2images, box_th=0.25):\n",
    "    \"\"\"\n",
    "    Function that performs inference and plots resulting b-boxes\n",
    "    \n",
    "    Args:\n",
    "      path2images: an array with pathes to images\n",
    "      box_th: (float) value that defines threshold for model prediction.\n",
    "      \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    for image_path in path2images:\n",
    "\n",
    "        print('Running inference for {}... '.format(image_path), end='')\n",
    "\n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "        \n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # All outputs are batches tensors.\n",
    "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "        # We're only interested in the first num_detections.\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                      for key, value in detections.items()}\n",
    "        \n",
    "        detections['num_detections'] = num_detections\n",
    "\n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = image_np.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=200,\n",
    "                min_score_thresh=box_th,\n",
    "                agnostic_mode=False,\n",
    "                line_thickness=5)\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.imshow(image_np_with_detections)\n",
    "        print('Done')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a few other supporting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(rects, thd=0.5):\n",
    "    \"\"\"\n",
    "    Filter rectangles\n",
    "    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n",
    "    thd - intersection threshold (intersection divides min square of rectange)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    remove = [False] * len(rects)\n",
    "\n",
    "    for i in range(0, len(rects) - 1):\n",
    "        if remove[i]:\n",
    "            continue\n",
    "        inter = [0.0] * len(rects)\n",
    "        for j in range(i, len(rects)):\n",
    "            if remove[j]:\n",
    "                continue\n",
    "            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n",
    "\n",
    "        max_prob = 0.0\n",
    "        max_idx = 0\n",
    "        for k in range(i, len(rects)):\n",
    "            if inter[k] >= thd:\n",
    "                if rects[k][1] > max_prob:\n",
    "                    max_prob = rects[k][1]\n",
    "                    max_idx = k\n",
    "\n",
    "        for k in range(i, len(rects)):\n",
    "            if (inter[k] >= thd) & (k != max_idx):\n",
    "                remove[k] = True\n",
    "\n",
    "    for k in range(0, len(rects)):\n",
    "        if not remove[k]:\n",
    "            out.append(rects[k])\n",
    "\n",
    "    boxes = [box[0] for box in out]\n",
    "    scores = [score[1] for score in out]\n",
    "    classes = [cls[2] for cls in out]\n",
    "    return boxes, scores, classes\n",
    "\n",
    "\n",
    "def intersection(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Calculates square of intersection of two rectangles\n",
    "    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n",
    "    return: square of intersection\n",
    "    \"\"\"\n",
    "    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n",
    "    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n",
    "    overlapArea = x_overlap * y_overlap;\n",
    "    return overlapArea\n",
    "\n",
    "\n",
    "def square(rect):\n",
    "    \"\"\"\n",
    "    Calculates square of rectangle\n",
    "    \"\"\"\n",
    "    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next function is the one that you can use to run inference and save results into a file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_as_raw_output(path2images,\n",
    "                            box_th = 0.25,\n",
    "                            nms_th = 0.5,\n",
    "                            to_file = False,\n",
    "                            data = None,\n",
    "                            path2dir = False):\n",
    "    \"\"\"\n",
    "    Function that performs inference and return filtered predictions\n",
    "    \n",
    "    Args:\n",
    "      path2images: an array with pathes to images\n",
    "      box_th: (float) value that defines threshold for model prediction. Consider 0.25 as a value.\n",
    "      nms_th: (float) value that defines threshold for non-maximum suppression. Consider 0.5 as a value.\n",
    "      to_file: (boolean). When passed as True => results are saved into a file. Writing format is\n",
    "      path2image + (x1abs, y1abs, x2abs, y2abs, score, conf) for box in boxes\n",
    "      data: (str) name of the dataset you passed in (e.g. test/validation)\n",
    "      path2dir: (str). Should be passed if path2images has only basenames. If full pathes provided => set False.\n",
    "      \n",
    "    Returs:\n",
    "      detections (dict): filtered predictions that model made\n",
    "    \"\"\"\n",
    "    print (f'Current data set is {data}')\n",
    "    print (f'Ready to start inference on {len(path2images)} images!')\n",
    "    \n",
    "    for image_path in tqdm(path2images):\n",
    "        \n",
    "        if path2dir: # if a path to a directory where images are stored was passed in\n",
    "            image_path = os.path.join(path2dir, image_path.strip())\n",
    "            \n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "        \n",
    "        # checking how many detections we got\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        \n",
    "        # filtering out detection in order to get only the one that are indeed detections\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        \n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "        \n",
    "        # defining what we need from the resulting detection dict that we got from model output\n",
    "        key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n",
    "        \n",
    "        # filtering out detection dict in order to get only boxes, classes and scores\n",
    "        detections = {key: value for key, value in detections.items() if key in key_of_interest}\n",
    "        \n",
    "        if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n",
    "            for key in key_of_interest:\n",
    "                scores = detections['detection_scores']\n",
    "                current_array = detections[key]\n",
    "                filtered_current_array = current_array[scores > box_th]\n",
    "                detections[key] = filtered_current_array\n",
    "        \n",
    "        if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n",
    "            # creating a zip object that will contain model output info as\n",
    "            output_info = list(zip(detections['detection_boxes'],\n",
    "                                   detections['detection_scores'],\n",
    "                                   detections['detection_classes']\n",
    "                                  )\n",
    "                              )\n",
    "            boxes, scores, classes = nms(output_info)\n",
    "            \n",
    "            detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n",
    "            detections['detection_scores'] = scores\n",
    "            detections['detection_classes'] = classes\n",
    "            \n",
    "        if to_file and data: # if saving to txt file was requested\n",
    "\n",
    "            image_h, image_w, _ = image_np.shape\n",
    "            file_name = f'pred_result_{data}.txt'\n",
    "            \n",
    "            line2write = list()\n",
    "            line2write.append(os.path.basename(image_path))\n",
    "            \n",
    "            with open(file_name, 'a+') as text_file:\n",
    "                # iterating over boxes\n",
    "                for b, s, c in zip(boxes, scores, classes):\n",
    "                    \n",
    "                    y1abs, x1abs = b[0] * image_h, b[1] * image_w\n",
    "                    y2abs, x2abs = b[2] * image_h, b[3] * image_w\n",
    "                    \n",
    "                    list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n",
    "                    line2append = ','.join([str(item) for item in list2append])\n",
    "                    \n",
    "                    line2write.append(line2append)\n",
    "                \n",
    "                line2write = ' '.join(line2write)\n",
    "                text_file.write(line2write + os.linesep)\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images/bus18.jpg', 'images/bus19.jpg', 'images/bus2.jpeg', 'images/bus24.jpg', 'images/bus25.jpg', 'images/truck14.jpg', 'images/truck15.jpg', 'images/van4.jpg', 'images/van5.jpg', 'images/video4img_1607.jpg', 'images/video4img_1608.jpg', 'images/video4img_1609.jpg', 'images/video4img_1623.jpg', 'images/video4img_1629.jpg', 'images/video4img_1632.jpg', 'images/video4img_1643.jpg', 'images/video4img_1648.jpg', 'images/video4img_1653.jpg', 'images/video4img_1655.jpg', 'images/video4img_1680.jpg', 'images/video4img_1681.jpg', 'images/video4img_1682.jpg', 'images/video4img_1718.jpg', 'images/video4img_1730.jpg', 'images/video4img_1731.jpg', 'images/video4img_1739.jpg', 'images/video4img_1744.jpg', 'images/video4img_1767.jpg', 'images/video4img_1772.jpg', 'images/video4img_1779.jpg', 'images/video4img_1790.jpg', 'images/video4img_1793.jpg', 'images/video4img_1794.jpg', 'images/video4img_1796.jpg', 'images/video4img_1814.jpg', 'images/video4img_1815.jpg', 'images/video4img_1823.jpg', 'images/video4img_1830.jpg', 'images/video4img_1834.jpg', 'images/video4img_1836.jpg', 'images/video4img_1838.jpg', 'images/video4img_1845.jpg', 'images/video4img_1847.jpg', 'images/video4img_1854.jpg', 'images/video4img_1858.jpg', 'images/video4img_1859.jpg', 'images/video4img_1860.jpg', 'images/video4img_1862.jpg', 'images/video4img_1867.jpg', 'images/video4img_1871.jpg', 'images/video4img_1879.jpg', 'images/video4img_1905.jpg', 'images/video4img_1908.jpg', 'images/video4img_1911.jpg', 'images/video4img_1925.jpg', 'images/video4img_1928.jpg', 'images/video4img_1937.jpg', 'images/video4img_1938.jpg', 'images/video4img_1953.jpg', 'images/video4img_1957.jpg', 'images/video4img_1959.jpg', 'images/video4img_1964.jpg', 'images/video4img_1967.jpg', 'images/video4img_1969.jpg', 'images/video4img_1970.jpg', 'images/video4img_1971.jpg', 'images/video4img_1972.jpg', 'images/video4img_1976.jpg', 'images/video4img_1977.jpg', 'images/video4img_1978.jpg', 'images/video4img_1987.jpg', 'images/video4img_1988.jpg', 'images/video4img_2004.jpg', 'images/video4img_2009.jpg', 'images/video4img_2028.jpg', 'images/video4img_2029.jpg', 'images/video4img_2041.jpg', 'images/video4img_2042.jpg', 'images/video4img_2052.jpg', 'images/video4img_2055.jpg', 'images/video4img_2056.jpg', 'images/video4img_2062.jpg', 'images/video4img_2063.jpg', 'images/video4img_2064.jpg', 'images/video4img_2072.jpg', 'images/video4img_2074.jpg', 'images/video4img_2076.jpg', 'images/video4img_2100.jpg', 'images/video4img_2115.jpg', 'images/video4img_2119.jpg', 'images/video4img_2120.jpg', 'images/video4img_2121.jpg', 'images/video4img_2131.jpg', 'images/video4img_2140.jpg', 'images/video4img_2141.jpg', 'images/video4img_2142.jpg', 'images/video4img_2145.jpg', 'images/video4img_2151.jpg', 'images/video4img_2154.jpg', 'images/video4img_2155.jpg', 'images/video4img_2156.jpg', 'images/video4img_2167.jpg', 'images/video4img_2179.jpg', 'images/video4img_2180.jpg', 'images/video4img_2195.jpg', 'images/video4img_2199.jpg', 'images/video4img_2200.jpg', 'images/video4img_2202.jpg', 'images/video4img_2208.jpg', 'images/video4img_2213.jpg', 'images/video4img_2233.jpg', 'images/video4img_2241.jpg', 'images/video4img_2242.jpg', 'images/video4img_2244.jpg', 'images/video4img_2245.jpg', 'images/video4img_2250.jpg', 'images/video4img_2251.jpg', 'images/video4img_2255.jpg', 'images/video4img_2260.jpg', 'images/video4img_2261.jpg', 'images/video4img_2272.jpg', 'images/video4img_2273.jpg', 'images/video4img_2277.jpg', 'images/video4img_2281.jpg', 'images/video4img_2282.jpg', 'images/video4img_2285.jpg', 'images/video4img_2286.jpg', 'images/video4img_2311.jpg', 'images/video4img_2314.jpg', 'images/video4img_2315.jpg', 'images/video4img_2316.jpg', 'images/video4img_2317.jpg', 'images/video4img_2320.jpg', 'images/video4img_2321.jpg', 'images/video4img_2326.jpg', 'images/video4img_2334.jpg', 'images/video4img_2337.jpg', 'images/video4img_2339.jpg', 'images/video4img_2354.jpg', 'images/video4img_2358.jpg', 'images/video4img_2360.jpg', 'images/video4img_2367.jpg', 'images/video4img_2368.jpg', 'images/video4img_2374.jpg', 'images/video4img_2381.jpg', 'images/video4img_2385.jpg', 'images/video4img_2390.jpg', 'images/video4img_2392.jpg', 'images/video4img_2393.jpg', 'images/video4img_2398.jpg', 'images/video4img_2404.jpg', 'images/video4img_2406.jpg', 'images/video4img_2407.jpg', 'images/video4img_2408.jpg', 'images/video4img_2415.jpg', 'images/video4img_2426.jpg', 'images/video4img_2427.jpg', 'images/video4img_2428.jpg', 'images/video4img_2429.jpg', 'images/video4img_2435.jpg', 'images/video4img_2436.jpg', 'images/video4img_2442.jpg', 'images/video4img_2453.jpg', 'images/video4img_2459.jpg', 'images/video4img_2461.jpg', 'images/video4img_2484.jpg', 'images/video4img_2485.jpg', 'images/video4img_2488.jpg', 'images/video4img_2493.jpg', 'images/video4img_2494.jpg', 'images/video4img_2500.jpg', 'images/video4img_2515.jpg', 'images/video4img_2525.jpg', 'images/video4img_2530.jpg', 'images/video4img_2532.jpg', 'images/video4img_2544.jpg', 'images/video4img_2545.jpg', 'images/video4img_2576.jpg', 'images/video4img_2577.jpg', 'images/video4img_2578.jpg', 'images/video4img_2579.jpg', 'images/video4img_2582.jpg', 'images/video4img_2584.jpg', 'images/video4img_2585.jpg', 'images/video4img_2588.jpg', 'images/video4img_2596.jpg', 'images/video4img_2597.jpg', 'images/video4img_2600.jpg', 'images/video4img_2606.jpg', 'images/video4img_2607.jpg', 'images/video4img_2610.jpg', 'images/video4img_2611.jpg', 'images/video4img_2612.jpg', 'images/video4img_2615.jpg', 'images/video4img_2618.jpg', 'images/video4img_2622.jpg', 'images/video4img_2625.jpg', 'images/video4img_2635.jpg', 'images/video4img_2637.jpg', 'images/video4img_2667.jpg', 'images/video4img_2684.jpg', 'images/video4img_2685.jpg', 'images/video4img_2686.jpg', 'images/video4img_2696.jpg', 'images/video4img_2699.jpg', 'images/video4img_2704.jpg', 'images/video4img_2706.jpg', 'images/video4img_2716.jpg', 'images/video4img_2732.jpg', 'images/video4img_2745.jpg', 'images/video4img_2750.jpg', 'images/video4img_2751.jpg', 'images/video4img_2752.jpg', 'images/video4img_2754.jpg', 'images/video4img_2755.jpg', 'images/video4img_2759.jpg', 'images/video4img_2760.jpg', 'images/video4img_2764.jpg', 'images/video4img_2765.jpg', 'images/video4img_2767.jpg', 'images/video4img_2776.jpg', 'images/video4img_2782.jpg', 'images/video4img_2787.jpg', 'images/video4img_2788.jpg', 'images/video4img_2790.jpg', 'images/video4img_2792.jpg', 'images/video4img_2795.jpg', 'images/video4img_2801.jpg', 'images/video4img_2820.jpg', 'images/video4img_2821.jpg', 'images/video4img_2825.jpg', 'images/video4img_2832.jpg', 'images/video4img_2836.jpg', 'images/video4img_2842.jpg', 'images/video4img_2856.jpg', 'images/video4img_2857.jpg', 'images/video4img_2884.jpg', 'images/video4img_2885.jpg', 'images/video4img_2887.jpg', 'images/video4img_2888.jpg', 'images/video4img_2890.jpg', 'images/video4img_2891.jpg', 'images/video4img_2892.jpg', 'images/video4img_2893.jpg', 'images/video4img_2905.jpg', 'images/video4img_2907.jpg', 'images/video4img_2908.jpg', 'images/video4img_2910.jpg', 'images/video4img_2911.jpg', 'images/video4img_2916.jpg', 'images/video4img_2928.jpg', 'images/video4img_2929.jpg', 'images/video4img_2934.jpg', 'images/video4img_2976.jpg', 'images/video4img_2983.jpg', 'images/video4img_2985.jpg', 'images/video4img_2988.jpg', 'images/video4img_3006.jpg', 'images/video4img_3015.jpg', 'images/video4img_3017.jpg', 'images/video4img_3018.jpg', 'images/video4img_3026.jpg', 'images/video4img_3032.jpg', 'images/video4img_3034.jpg', 'images/video4img_3035.jpg', 'images/video4img_3036.jpg', 'images/video4img_3037.jpg', 'images/video4img_3048.jpg', 'images/video4img_3049.jpg', 'images/video4img_3059.jpg', 'images/video4img_3060.jpg', 'images/video4img_3064.jpg', 'images/video4img_3070.jpg', 'images/video4img_3071.jpg', 'images/video4img_3072.jpg', 'images/video4img_3073.jpg', 'images/video4img_3077.jpg', 'images/video4img_3078.jpg', 'images/video4img_3088.jpg', 'images/video4img_3090.jpg', 'images/video4img_3094.jpg', 'images/video4img_3098.jpg', 'images/video4img_3099.jpg', 'images/video4img_3101.jpg', 'images/video4img_3102.jpg', 'images/video4img_3103.jpg', 'images/video4img_3106.jpg', 'images/video4img_3108.jpg', 'images/video4img_3110.jpg', 'images/video4img_3111.jpg', 'images/video4img_3118.jpg', 'images/video4img_3119.jpg', 'images/video4img_3127.jpg', 'images/video4img_3130.jpg', 'images/video4img_3135.jpg', 'images/video4img_3146.jpg', 'images/video4img_3156.jpg', 'images/video4img_3158.jpg', 'images/video4img_3159.jpg', 'images/video4img_3161.jpg', 'images/video4img_3176.jpg', 'images/video4img_3190.jpg', 'images/video4img_3195.jpg', 'images/video4img_3201.jpg', 'images/video4img_3210.jpg', 'images/video4img_3230.jpg', 'images/video4img_3231.jpg', 'images/video4img_3236.jpg', 'images/video4img_3241.jpg', 'images/video4img_3242.jpg', 'images/video4img_3244.jpg', 'images/video4img_3245.jpg', 'images/video4img_3247.jpg', 'images/video4img_3248.jpg', 'images/video4img_3260.jpg', 'images/video4img_3261.jpg', 'images/video4img_3263.jpg', 'images/video4img_3269.jpg', 'images/video4img_3274.jpg', 'images/video4img_3292.jpg', 'images/video4img_3297.jpg', 'images/video4img_3298.jpg', 'images/video4img_3299.jpg', 'images/video4img_3302.jpg', 'images/video4img_3304.jpg', 'images/video4img_3316.jpg', 'images/video4img_3321.jpg', 'images/video4img_3322.jpg', 'images/video4img_3324.jpg', 'images/video4img_3326.jpg', 'images/video4img_3327.jpg', 'images/video4img_3330.jpg', 'images/video4img_3331.jpg', 'images/video4img_3332.jpg', 'images/video4img_3337.jpg', 'images/video4img_3341.jpg', 'images/video4img_3343.jpg', 'images/video4img_3344.jpg', 'images/video4img_3345.jpg', 'images/video4img_3348.jpg', 'images/video4img_3356.jpg', 'images/video4img_3357.jpg', 'images/video4img_3360.jpg', 'images/video4img_3368.jpg', 'images/video4img_3372.jpg', 'images/video4img_3375.jpg', 'images/video4img_3376.jpg', 'images/video4img_3386.jpg', 'images/video4img_3393.jpg', 'images/video4img_3416.jpg', 'images/video4img_3417.jpg', 'images/video4img_3427.jpg', 'images/video4img_3428.jpg', 'images/video4img_3429.jpg', 'images/video4img_3430.jpg', 'images/video4img_3436.jpg', 'images/video4img_3463.jpg', 'images/video4img_3476.jpg', 'images/video4img_3477.jpg', 'images/video4img_3480.jpg', 'images/video4img_3481.jpg', 'images/video4img_3482.jpg', 'images/video4img_3486.jpg', 'images/video4img_3487.jpg', 'images/video4img_3491.jpg', 'images/video4img_3492.jpg', 'images/video4img_3493.jpg', 'images/video4img_3502.jpg', 'images/video4img_3503.jpg', 'images/video4img_3506.jpg', 'images/video4img_3507.jpg', 'images/video4img_3513.jpg', 'images/video4img_3517.jpg', 'images/video4img_3518.jpg', 'images/video4img_3521.jpg', 'images/video4img_3539.jpg', 'images/video4img_3540.jpg', 'images/video4img_3542.jpg', 'images/video4img_3567.jpg', 'images/video4img_3571.jpg', 'images/video4img_3578.jpg', 'images/video4img_3590.jpg', 'images/video4img_3591.jpg', 'images/video4img_3595.jpg', 'images/video4img_3596.jpg', 'images/video4img_3599.jpg', 'images/video4img_3612.jpg', 'images/video4img_3630.jpg', 'images/video4img_3631.jpg', 'images/video4img_3641.jpg', 'images/video4img_3642.jpg', 'images/video4img_3643.jpg', 'images/video4img_3660.jpg', 'images/video4img_3661.jpg', 'images/video4img_3672.jpg', 'images/video4img_3675.jpg', 'images/video4img_3679.jpg', 'images/video4img_3680.jpg', 'images/video4img_3682.jpg', 'images/video4img_3691.jpg', 'images/video4img_3694.jpg', 'images/video4img_3700.jpg', 'images/video4img_3702.jpg', 'images/video4img_3703.jpg', 'images/video4img_3712.jpg', 'images/video4img_3716.jpg', 'images/video4img_3730.jpg', 'images/video4img_3732.jpg', 'images/video4img_3736.jpg', 'images/video4img_3745.jpg', 'images/video4img_3746.jpg', 'images/video4img_3750.jpg', 'images/video4img_3758.jpg', 'images/video4img_3765.jpg', 'images/video4img_3767.jpg', 'images/video4img_3768.jpg', 'images/video4img_3772.jpg', 'images/video4img_3774.jpg', 'images/video4img_3776.jpg', 'images/video4img_3779.jpg', 'images/video4img_3780.jpg', 'images/video4img_3786.jpg', 'images/video4img_3807.jpg', 'images/video4img_3811.jpg', 'images/video4img_3812.jpg', 'images/video4img_3816.jpg', 'images/video4img_3832.jpg', 'images/video4img_3833.jpg', 'images/video4img_3835.jpg', 'images/video4img_3837.jpg', 'images/video4img_3839.jpg', 'images/video4img_3844.jpg', 'images/video4img_3847.jpg', 'images/video4img_3848.jpg', 'images/video4img_3850.jpg', 'images/video4img_3852.jpg', 'images/video4img_3853.jpg', 'images/video4img_3854.jpg', 'images/video4img_3855.jpg', 'images/video4img_3858.jpg', 'images/video4img_3863.jpg', 'images/video4img_3868.jpg', 'images/video4img_3869.jpg', 'images/video4img_3871.jpg', 'images/video4img_3872.jpg', 'images/video4img_3873.jpg', 'images/video4img_3874.jpg', 'images/video4img_3879.jpg', 'images/video4img_3890.jpg', 'images/video4img_3893.jpg', 'images/video4img_3901.jpg', 'images/video4img_3903.jpg', 'images/video4img_3907.jpg', 'images/video4img_3909.jpg', 'images/video4img_3910.jpg', 'images/video4img_3911.jpg', 'images/video4img_3913.jpg', 'images/video4img_3920.jpg', 'images/video4img_3922.jpg', 'images/video4img_3923.jpg', 'images/video4img_3926.jpg', 'images/video4img_3927.jpg', 'images/video4img_3929.jpg', 'images/video4img_3931.jpg', 'images/video4img_3938.jpg', 'images/video4img_3939.jpg', 'images/video4img_3940.jpg', 'images/video4img_3941.jpg', 'images/video4img_3967.jpg', 'images/video4img_3970.jpg', 'images/video4img_3979.jpg', 'images/video4img_3983.jpg', 'images/video4img_3991.jpg', 'images/video4img_3993.jpg', 'images/video4img_3994.jpg', 'images/video4img_4007.jpg', 'images/video4img_4011.jpg', 'images/video4img_4013.jpg', 'images/video4img_4019.jpg', 'images/video4img_4025.jpg', 'images/video4img_4030.jpg', 'images/video4img_4042.jpg', 'images/video4img_4043.jpg', 'images/video4img_4045.jpg', 'images/video4img_4046.jpg', 'images/video4img_4049.jpg', 'images/video4img_4052.jpg', 'images/video4img_4062.jpg', 'images/video4img_4064.jpg', 'images/video4img_4065.jpg', 'images/video4img_4077.jpg', 'images/video4img_4081.jpg', 'images/video4img_4082.jpg', 'images/video4img_4094.jpg', 'images/video4img_4114.jpg', 'images/video4img_4121.jpg', 'images/video4img_4123.jpg', 'images/video4img_4125.jpg', 'images/video4img_4126.jpg', 'images/video4img_4129.jpg', 'images/video4img_4131.jpg', 'images/video4img_4134.jpg', 'images/video4img_4135.jpg', 'images/video4img_4136.jpg', 'images/video4img_4137.jpg', 'images/video4img_4138.jpg', 'images/video4img_4139.jpg', 'images/video4img_4143.jpg', 'images/video4img_4145.jpg', 'images/video4img_4146.jpg', 'images/video4img_4149.jpg', 'images/video4img_4150.jpg', 'images/video4img_4151.jpg', 'images/video4img_4152.jpg', 'images/video4img_4153.jpg', 'images/video4img_4154.jpg', 'images/video4img_4165.jpg', 'images/video4img_4168.jpg', 'images/video4img_4183.jpg', 'images/video4img_4191.jpg', 'images/video4img_4192.jpg', 'images/video4img_4193.jpg', 'images/video4img_4198.jpg', 'images/video4img_4217.jpg', 'images/video4img_4219.jpg', 'images/video4img_4220.jpg', 'images/video4img_4226.jpg', 'images/video4img_4228.jpg', 'images/video4img_4229.jpg', 'images/video4img_4235.jpg', 'images/video4img_4236.jpg', 'images/video4img_4243.jpg', 'images/video4img_4244.jpg', 'images/video4img_4253.jpg', 'images/video4img_4258.jpg', 'images/video4img_4261.jpg', 'images/video4img_4262.jpg', 'images/video4img_4263.jpg', 'images/video4img_4270.jpg', 'images/video4img_4275.jpg', 'images/video4img_4279.jpg', 'images/video4img_4284.jpg', 'images/video4img_4288.jpg', 'images/video4img_4289.jpg', 'images/video4img_4293.jpg', 'images/video4img_4295.jpg', 'images/video4img_4297.jpg', 'images/video4img_4300.jpg', 'images/video4img_4302.jpg', 'images/video5img_2700.jpg', 'images/video5img_2701.jpg', 'images/video5img_2705.jpg', 'images/video5img_2706.jpg', 'images/video5img_2707.jpg', 'images/video5img_2708.jpg', 'images/video5img_2709.jpg', 'images/video5img_2739.jpg', 'images/video5img_2740.jpg', 'images/video5img_2742.jpg', 'images/video5img_2743.jpg', 'images/video5img_2753.jpg', 'images/video5img_2754.jpg', 'images/video5img_2756.jpg', 'images/video5img_2757.jpg', 'images/video5img_2759.jpg', 'images/video5img_2760.jpg', 'images/video5img_2774.jpg', 'images/video5img_2775.jpg', 'images/video5img_2777.jpg', 'images/video5img_2778.jpg', 'images/video5img_2779.jpg', 'images/video5img_2788.jpg', 'images/video5img_2789.jpg', 'images/video5img_2790.jpg', 'images/video5img_2796.jpg', 'images/video5img_2797.jpg', 'images/video5img_2826.jpg', 'images/video5img_2827.jpg', 'images/video5img_2828.jpg', 'images/video5img_2831.jpg', 'images/video5img_2832.jpg', 'images/video5img_2873.jpg', 'images/video5img_2874.jpg', 'images/video5img_2875.jpg', 'images/video5img_2877.jpg', 'images/video5img_2879.jpg', 'images/video5img_2885.jpg', 'images/video5img_2886.jpg', 'images/video5img_2895.jpg', 'images/video5img_2896.jpg', 'images/video5img_2920.jpg', 'images/video5img_2921.jpg', 'images/video5img_2922.jpg', 'images/video5img_2923.jpg', 'images/video5img_2947.jpg', 'images/video5img_2948.jpg', 'images/video5img_2949.jpg', 'images/video5img_2955.jpg', 'images/video5img_2956.jpg', 'images/video5img_2961.jpg', 'images/video5img_2962.jpg', 'images/video5img_2975.jpg', 'images/video5img_2976.jpg', 'images/video5img_2993.jpg', 'images/video5img_2994.jpg', 'images/video5img_2999.jpg', 'images/video5img_3000.jpg', 'images/video5img_3006.jpg', 'images/video5img_3007.jpg', 'images/video5img_3008.jpg', 'images/video5img_3009.jpg', 'images/video5img_3010.jpg', 'images/video5img_3011.jpg', 'images/video5img_3018.jpg', 'images/video5img_3019.jpg', 'images/video5img_3029.jpg', 'images/video5img_3030.jpg', 'images/video5img_3042.jpg', 'images/video5img_3043.jpg', 'images/video5img_3057.jpg', 'images/video5img_3058.jpg', 'images/video5img_3083.jpg', 'images/video5img_3084.jpg', 'images/video5img_3085.jpg', 'images/video5img_3122.jpg', 'images/video5img_3123.jpg', 'images/video5img_3124.jpg', 'images/video5img_3132.jpg', 'images/video5img_3133.jpg', 'images/video5img_3134.jpg', 'images/video5img_3135.jpg', 'images/video5img_3136.jpg', 'images/video5img_3137.jpg', 'images/video5img_3138.jpg', 'images/video5img_3139.jpg', 'images/video5img_3140.jpg', 'images/video5img_3141.jpg', 'images/video5img_3154.jpg', 'images/video5img_3171.jpg', 'images/video5img_3172.jpg', 'images/video5img_3196.jpg', 'images/video5img_3202.jpg', 'images/video5img_3203.jpg', 'images/video5img_3207.jpg', 'images/video5img_3213.jpg', 'images/video5img_3215.jpg', 'images/video5img_3218.jpg', 'images/video5img_3219.jpg', 'images/video5img_3222.jpg', 'images/video5img_3223.jpg', 'images/video5img_3224.jpg', 'images/video5img_3227.jpg', 'images/video5img_3228.jpg', 'images/video5img_3229.jpg', 'images/video5img_3230.jpg', 'images/video5img_3232.jpg', 'images/video5img_3233.jpg', 'images/video5img_3253.jpg', 'images/video5img_3254.jpg', 'images/video5img_3255.jpg', 'images/video5img_3256.jpg', 'images/video5img_3257.jpg', 'images/video5img_3258.jpg', 'images/video5img_3259.jpg', 'images/video5img_3260.jpg', 'images/video5img_3261.jpg', 'images/video5img_3262.jpg', 'images/video5img_3263.jpg', 'images/video5img_3264.jpg', 'images/video5img_3265.jpg', 'images/video5img_3266.jpg', 'images/video5img_3267.jpg', 'images/video5img_3268.jpg', 'images/video5img_3269.jpg', 'images/video5img_3270.jpg', 'images/video5img_3271.jpg', 'images/video5img_3272.jpg', 'images/video5img_3273.jpg', 'images/video5img_3274.jpg', 'images/video5img_3275.jpg', 'images/video5img_3276.jpg', 'images/video5img_3277.jpg', 'images/video5img_3281.jpg', 'images/video5img_3350.jpg', 'images/video5img_3351.jpg', 'images/video5img_3352.jpg', 'images/video5img_3353.jpg', 'images/video5img_3354.jpg', 'images/video5img_3355.jpg', 'images/video5img_3356.jpg', 'images/video5img_3357.jpg', 'images/video5img_3358.jpg', 'images/video5img_3359.jpg', 'images/video5img_3360.jpg', 'images/video5img_3361.jpg', 'images/video5img_3362.jpg', 'images/video5img_3363.jpg', 'images/video5img_3364.jpg', 'images/video5img_3365.jpg', 'images/video5img_3374.jpg', 'images/video5img_3383.jpg', 'images/video5img_3384.jpg', 'images/video5img_3387.jpg', 'images/video5img_3388.jpg', 'images/video5img_3389.jpg', 'images/video5img_3392.jpg', 'images/video5img_3393.jpg', 'images/video5img_3405.jpg', 'images/video5img_3406.jpg', 'images/video5img_3407.jpg', 'images/video5img_3408.jpg', 'images/video5img_3409.jpg', 'images/video5img_3414.jpg', 'images/video5img_3415.jpg', 'images/video5img_3425.jpg', 'images/video5img_3426.jpg', 'images/video5img_3447.jpg', 'images/video5img_3448.jpg', 'images/video5img_3449.jpg', 'images/video5img_3454.jpg', 'images/video5img_3455.jpg', 'images/video5img_3458.jpg', 'images/video5img_3459.jpg', 'images/video5img_3474.jpg', 'images/video5img_3475.jpg', 'images/video5img_3476.jpg', 'images/video5img_3477.jpg', 'images/video5img_3481.jpg', 'images/video5img_3482.jpg', 'images/video5img_3488.jpg', 'images/video5img_3489.jpg', 'images/video5img_3526.jpg', 'images/video5img_3527.jpg', 'images/video5img_3537.jpg', 'images/video5img_3538.jpg', 'images/video5img_3539.jpg', 'images/video5img_3574.jpg', 'images/video5img_3576.jpg', 'images/video5img_3577.jpg', 'images/video5img_3578.jpg', 'images/video5img_3584.jpg']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for images/bus18.jpg... Done\n",
      "Running inference for images/bus19.jpg... Done\n",
      "Running inference for images/bus2.jpeg... Done\n",
      "Running inference for images/bus24.jpg... Done\n",
      "Running inference for images/bus25.jpg... Done\n",
      "Running inference for images/truck14.jpg... Done\n",
      "Running inference for images/truck15.jpg... Done\n",
      "Running inference for images/van4.jpg... Done\n",
      "Running inference for images/van5.jpg... Done\n",
      "Running inference for images/video4img_1607.jpg... Done\n",
      "Running inference for images/video4img_1608.jpg... Done\n",
      "Running inference for images/video4img_1609.jpg... Done\n",
      "Running inference for images/video4img_1623.jpg... Done\n",
      "Running inference for images/video4img_1629.jpg... Done\n",
      "Running inference for images/video4img_1632.jpg... Done\n",
      "Running inference for images/video4img_1643.jpg... Done\n",
      "Running inference for images/video4img_1648.jpg... Done\n",
      "Running inference for images/video4img_1653.jpg... Done\n",
      "Running inference for images/video4img_1655.jpg... Done\n",
      "Running inference for images/video4img_1680.jpg... Done\n",
      "Running inference for images/video4img_1681.jpg... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Temp/ipykernel_22496/3956868624.py:48: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure(figsize=(15,10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Running inference for images/video4img_1682.jpg... Done\n",
      "Running inference for images/video4img_1718.jpg... Done\n",
      "Running inference for images/video4img_1730.jpg... Done\n",
      "Running inference for images/video4img_1731.jpg... Done\n",
      "Running inference for images/video4img_1739.jpg... Done\n",
      "Running inference for images/video4img_1744.jpg... Done\n",
      "Running inference for images/video4img_1767.jpg... Done\n",
      "Running inference for images/video4img_1772.jpg... Done\n",
      "Running inference for images/video4img_1779.jpg... Done\n",
      "Running inference for images/video4img_1790.jpg... Done\n",
      "Running inference for images/video4img_1793.jpg... Done\n",
      "Running inference for images/video4img_1794.jpg... Done\n",
      "Running inference for images/video4img_1796.jpg... Done\n",
      "Running inference for images/video4img_1814.jpg... Done\n",
      "Running inference for images/video4img_1815.jpg... Done\n",
      "Running inference for images/video4img_1823.jpg... Done\n",
      "Running inference for images/video4img_1830.jpg... Done\n",
      "Running inference for images/video4img_1834.jpg... Done\n",
      "Running inference for images/video4img_1836.jpg... Done\n",
      "Running inference for images/video4img_1838.jpg... Done\n",
      "Running inference for images/video4img_1845.jpg... Done\n",
      "Running inference for images/video4img_1847.jpg... Done\n",
      "Running inference for images/video4img_1854.jpg... Done\n",
      "Running inference for images/video4img_1858.jpg... Done\n",
      "Running inference for images/video4img_1859.jpg... Done\n",
      "Running inference for images/video4img_1860.jpg... Done\n",
      "Running inference for images/video4img_1862.jpg... Done\n",
      "Running inference for images/video4img_1867.jpg... Done\n",
      "Running inference for images/video4img_1871.jpg... Done\n",
      "Running inference for images/video4img_1879.jpg... Done\n",
      "Running inference for images/video4img_1905.jpg... Done\n",
      "Running inference for images/video4img_1908.jpg... Done\n",
      "Running inference for images/video4img_1911.jpg... Done\n",
      "Running inference for images/video4img_1925.jpg... Done\n",
      "Running inference for images/video4img_1928.jpg... Done\n",
      "Running inference for images/video4img_1937.jpg... Done\n",
      "Running inference for images/video4img_1938.jpg... Done\n",
      "Running inference for images/video4img_1953.jpg... Done\n",
      "Running inference for images/video4img_1957.jpg... Done\n",
      "Running inference for images/video4img_1959.jpg... Done\n",
      "Running inference for images/video4img_1964.jpg... Done\n",
      "Running inference for images/video4img_1967.jpg... Done\n",
      "Running inference for images/video4img_1969.jpg... Done\n",
      "Running inference for images/video4img_1970.jpg... Done\n",
      "Running inference for images/video4img_1971.jpg... Done\n",
      "Running inference for images/video4img_1972.jpg... Done\n",
      "Running inference for images/video4img_1976.jpg... Done\n",
      "Running inference for images/video4img_1977.jpg... Done\n",
      "Running inference for images/video4img_1978.jpg... Done\n",
      "Running inference for images/video4img_1987.jpg... Done\n",
      "Running inference for images/video4img_1988.jpg... Done\n",
      "Running inference for images/video4img_2004.jpg... Done\n",
      "Running inference for images/video4img_2009.jpg... Done\n",
      "Running inference for images/video4img_2028.jpg... Done\n",
      "Running inference for images/video4img_2029.jpg... Done\n",
      "Running inference for images/video4img_2041.jpg... Done\n",
      "Running inference for images/video4img_2042.jpg... Done\n",
      "Running inference for images/video4img_2052.jpg... Done\n",
      "Running inference for images/video4img_2055.jpg... Done\n",
      "Running inference for images/video4img_2056.jpg... Done\n",
      "Running inference for images/video4img_2062.jpg... Done\n",
      "Running inference for images/video4img_2063.jpg... Done\n",
      "Running inference for images/video4img_2064.jpg... Done\n",
      "Running inference for images/video4img_2072.jpg... Done\n",
      "Running inference for images/video4img_2074.jpg... Done\n",
      "Running inference for images/video4img_2076.jpg... Done\n",
      "Running inference for images/video4img_2100.jpg... Done\n",
      "Running inference for images/video4img_2115.jpg... Done\n",
      "Running inference for images/video4img_2119.jpg... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22496/4289469641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0minference_with_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2images\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpath2image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22496/3956868624.py\u001b[0m in \u001b[0;36minference_with_plot\u001b[1;34m(path2images, box_th)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# All outputs are batches tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22496/833259856.py\u001b[0m in \u001b[0;36mdetect_fn\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetection_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprediction_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetection_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetection_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\meta_architectures\\ssd_meta_arch.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, preprocessed_inputs, true_image_shapes)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_anchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbox_list_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxlist_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_predictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m       \u001b[0mpredictor_results_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m       with slim.arg_scope([slim.batch_norm],\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1096\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_call_info_injected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\core\\box_predictor.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, image_features, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m           \u001b[0mfeature\u001b[0m \u001b[0mmap\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \"\"\"\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\predictors\\convolutional_keras_box_predictor.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, image_features, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m       \u001b[1;31m# Apply box tower layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m       box_tower_feature = _apply_layers(\n\u001b[0m\u001b[0;32m    480\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_tower_layers_for_heads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBOX_ENCODINGS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m           image_feature)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\predictors\\convolutional_keras_box_predictor.py\u001b[0m in \u001b[0;36m_apply_layers\u001b[1;34m(base_tower_layers, image_feature)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_tower_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbase_tower_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[0mimage_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mimage_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1096\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_call_info_injected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\core\\freezable_batch_norm.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=g-bool-id-comparison\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreezableBatchNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[1;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    621\u001b[0m       \u001b[1;31m# pylint: enable=g-long-lambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m     output, mean, variance = control_flow_util.smart_cond(\n\u001b[0m\u001b[0;32m    624\u001b[0m         training, train_op, _fused_batch_norm_inference)\n\u001b[0;32m    625\u001b[0m     \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_add_or_remove_bessels_correction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\control_flow_util.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m    103\u001b[0m     return tf.cond(\n\u001b[0;32m    104\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m--> 105\u001b[1;33m   return tf.__internal__.smart_cond.smart_cond(\n\u001b[0m\u001b[0;32m    106\u001b[0m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm_inference\u001b[1;34m()\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fused_batch_norm_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m       return tf.compat.v1.nn.fused_batch_norm(\n\u001b[0m\u001b[0;32m    606\u001b[0m           \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m           \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path2image = []\n",
    "    d = \"images/\"\n",
    "    for path in os.listdir(d):\n",
    "        full_path = os.path.join(d, path)\n",
    "        path2image.append(full_path)\n",
    "    \n",
    "    print(path2image)\n",
    "    inference_with_plot(path2images= path2image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])\n",
    "\n",
    "IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'test', 'livelong.02533422-940e-11eb-9dbd-5cf3709bbcc6.jpg')\n",
    "\n",
    "img = cv2.imread(IMAGE_PATH)\n",
    "image_np = np.array(img)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "              for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5,\n",
    "            min_score_thresh=.8,\n",
    "            agnostic_mode=False)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22496/3312639707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_FRAME_WIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_FRAME_HEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.8,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "neptune": {
   "notebookId": "7c618cd5-39ec-46c6-bee7-0cfe5297f22a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
